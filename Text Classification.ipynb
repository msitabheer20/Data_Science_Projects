{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac660a70",
   "metadata": {},
   "source": [
    "# Project: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8019f79",
   "metadata": {},
   "source": [
    "## Neccessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78ef4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import enchant\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19af99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the data on which Naive Bayes is to be used\n",
    "\n",
    "newsgroup = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59b8ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data between the test and train\n",
    "\n",
    "x_train, x_test, y_train, y_test=train_test_split(newsgroup.data, newsgroup.target, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb9232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Making a list of all the stopwords present in the English language \n",
    "# 2. I have written all the stopwords in a text file and then loading it here to make the document\n",
    "# look clean\n",
    "\n",
    "stops = np.loadtxt(\"data/stop_words_english.txt\", dtype=str, delimiter=\" \")\n",
    "stopwords = list(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac71ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 'find_valid_words_in_text_list' is function which return a dictionary with all words \n",
    "# used in the training dataset with its frequency\n",
    "# 2. I have lowercase all the words in the training data set to have a consistent dictionary\n",
    "# 3. I have imported 'enchant' to check if the word in the document is valid english words or not\n",
    "# 4. I have excluded the digits in the data because they give very little info about the classification\n",
    "# 5. I have excluded words with length less that 3 to have more informative words in dictionary\n",
    "\n",
    "def find_valid_words_in_text_list(txt_list, stopwords) :\n",
    "    wordsList = []\n",
    "    for i in range(len(txt_list)) :\n",
    "        res = re.findall(r'\\w+', txt_list[i])\n",
    "        wordsList.extend(res)\n",
    "        \n",
    "    d = enchant.Dict('en-US')\n",
    "\n",
    "    wordsFreq = {}\n",
    "    for word in wordsList :\n",
    "        if (word.lower() in wordsFreq) or ((word.lower() not in stopwords) \n",
    "                                       and (word.isdigit() is False) \n",
    "                                       and (d.check(word)) \n",
    "                                       and len(word)!= 1 and len(word)!=2) :\n",
    "            wordsFreq[word.lower()] = wordsFreq.get(word.lower(), 0) + 1\n",
    "            \n",
    "    return wordsFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085c9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsFreq = find_valid_words_in_text_list(x_test, stopwords)\n",
    "\n",
    "# reversing the dictionary according to the values to get most frequent words at starting\n",
    "\n",
    "desc_wordsFreq = sorted(wordsFreq.items(), key = lambda x : x[1], reverse=True)\n",
    "desc_wordsFreq = dict(desc_wordsFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "707728af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'words' is a list containing all the words used in training data set\n",
    "# 'freq' is a list conataining the frequency of those words\n",
    "\n",
    "words=np.array([i for i in desc_wordsFreq.keys()])\n",
    "freq=np.array([i for i in desc_wordsFreq.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ef6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'gen_data_table' function return the 2d array where columns represent 'features' or 'words'\n",
    "# that are selected from the training data set text documents and rows represent the number of\n",
    "# times those 'features' or 'words' appear in each document \n",
    "\n",
    "def gen_data_table(x_data, features):\n",
    "    \n",
    "    word_feature_map = np.zeros((len(x_data), len(features)))\n",
    "    \n",
    "    for i in range(len(x_data)):\n",
    "        current_words = re.findall(r'\\w+', x_data[i])\n",
    "        tmpdct = {}\n",
    "        for word in current_words :\n",
    "            tmpdct[word] = tmpdct.get(word, 0) + 1\n",
    "            \n",
    "        for j in range(len(features)) :\n",
    "            if features[j] in tmpdct:\n",
    "                word_feature_map[i][j] = tmpdct[features[j]]\n",
    "                \n",
    "    return word_feature_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b28ed8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting 4000 features for the classification\n",
    "\n",
    "features = words[0:4000]\n",
    "\n",
    "x_train_2d = gen_data_table(x_train, features)\n",
    "\n",
    "x_test_2d = gen_data_table(x_test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5378840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing scikit learn's Naive Bayes' algorithm\n",
    "# MultinomialNB is used because multiple features are present in the training set\n",
    "\n",
    "clf=MultinomialNB()\n",
    "clf.fit(x_train_2d, y_train)\n",
    "clf.score(x_test_2d, y_test)\n",
    "y_pred_func = clf.predict(x_test_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbce9ad",
   "metadata": {},
   "source": [
    "## Own Implementation Of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99d0226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The fit function takes the training data as input and creates a dictionary of dictionries \n",
    "# with name 'result'\n",
    "# 2. The base dictionary contains all the possible classes in which to classify our text doc\n",
    "# 3. The second level dictionary contains how many times a particular feature is coming\n",
    "# corresponding to a particular class\n",
    "# 4. A 'total' key in included to store the total number of words that occurs in a particular class\n",
    "\n",
    "def fit(x_train, y_train):\n",
    "    result = {}\n",
    "    all_classes = set(y_train)\n",
    "    for current_class in all_classes:\n",
    "        \n",
    "        result[current_class] = {}\n",
    "        x_train_current = x_train[y_train == current_class]\n",
    "\n",
    "        result[current_class]['total']=0\n",
    "\n",
    "        for j in range(len(features)):\n",
    "            result[current_class][features[j]]=x_train_current[:, j].sum()\n",
    "            \n",
    "            result[current_class]['total'] += result[current_class][features[j]]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "009a75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.'probaility' function first stores all the probabilities for each word in features which is \n",
    "# available in the current document and then return the logarithmic sum of all these probabilities\n",
    "# for that particular document.\n",
    "# 2. skipping those features whose value corresponding to the current document is 0, \n",
    "# that is, they are not present\n",
    "# 3. 'numerator' - it denotes that how many times that ith feature is occuring in \"current_class\"\n",
    "#this is the numerator of our NON LOGARITHMIC PROBABILITY\n",
    "# 4. 'denominator' - it denotes that what is the total number of words in the current class.\n",
    "# this is the denominator of our NON LOGARITHMIC PROBABILITY\n",
    "# 5. 'proba' - stores the logarithmic probability which also includes the laplace crrection.\n",
    "# here \"1\" with the numerator and len(x) with denominator denotes the laplace correction.\n",
    "\n",
    "def probability(dictionary, x, current_class):\n",
    "    probas_for_each_word=[]\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        if x[i]!=0:\n",
    "\n",
    "            numerator=dictionary[current_class][features[i]]\n",
    "            \n",
    "            denominator=dictionary[current_class]['total']\n",
    "            \n",
    "            proba=np.log((numerator+1)/(denominator+len(x)))\n",
    "\n",
    "            probas_for_each_word.append(proba)\n",
    "\n",
    "    return sum(probas_for_each_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a58e041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 'Predict_single' function returns the best class to which a particular text document best\n",
    "# belongs to according to the maximum probability score\n",
    "# 2. the loop inside this function finds out the probability of a document corresponding to \n",
    "# each class if the probability of current class is better than the best probability then \n",
    "# it will update the best probability and best_class accordingly\n",
    "\n",
    "def predict_single(dictionary, x):\n",
    "    classes = dictionary.keys()\n",
    "\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    \n",
    "    first_run = True\n",
    "\n",
    "    for current_class in classes:\n",
    "        #iterating through each and every class in all possible classes.\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        #p_current_class denotes the probability of current class.\n",
    "        if (first_run or p_current_class > best_p):\n",
    "    \n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "\n",
    "        first_run = False\n",
    "\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12b5dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'predict' function return a list with the predicted classes or all testing data set\n",
    "\n",
    "def predict(x_test, dictionary):\n",
    "    y_pred=[]\n",
    "    \n",
    "    for doc in x_test:\n",
    "        \n",
    "        y_pred.append(predict_single(dictionary, doc))\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53bbd250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting training data\n",
    "\n",
    "dictionary=fit(x_train_2d, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d922b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted=predict(x_test_2d, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec024e",
   "metadata": {},
   "source": [
    "## Comparing MultinomialNB with our own Implementation of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d721b73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.79       114\n",
      "           1       0.66      0.72      0.69       152\n",
      "           2       0.67      0.72      0.69       139\n",
      "           3       0.64      0.68      0.66       152\n",
      "           4       0.67      0.67      0.67       138\n",
      "           5       0.78      0.75      0.76       153\n",
      "           6       0.75      0.78      0.76       147\n",
      "           7       0.74      0.80      0.77       137\n",
      "           8       0.79      0.85      0.82       131\n",
      "           9       0.84      0.87      0.85       135\n",
      "          10       0.88      0.88      0.88       136\n",
      "          11       0.94      0.90      0.92       145\n",
      "          12       0.77      0.65      0.71       157\n",
      "          13       0.90      0.91      0.90       151\n",
      "          14       0.91      0.83      0.87       155\n",
      "          15       0.80      0.84      0.82       159\n",
      "          16       0.81      0.86      0.83       140\n",
      "          17       0.84      0.81      0.83       149\n",
      "          18       0.75      0.69      0.72       138\n",
      "          19       0.65      0.59      0.62       101\n",
      "\n",
      "    accuracy                           0.78      2829\n",
      "   macro avg       0.78      0.78      0.78      2829\n",
      "weighted avg       0.78      0.78      0.78      2829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing the classification report for inbuilt naive bayes classifier.\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65a9eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79       114\n",
      "           1       0.58      0.72      0.64       152\n",
      "           2       0.65      0.75      0.70       139\n",
      "           3       0.58      0.70      0.64       152\n",
      "           4       0.60      0.63      0.62       138\n",
      "           5       0.78      0.67      0.72       153\n",
      "           6       0.76      0.69      0.73       147\n",
      "           7       0.72      0.80      0.76       137\n",
      "           8       0.86      0.79      0.83       131\n",
      "           9       0.85      0.87      0.86       135\n",
      "          10       0.90      0.90      0.90       136\n",
      "          11       0.91      0.94      0.93       145\n",
      "          12       0.81      0.56      0.66       157\n",
      "          13       0.93      0.90      0.92       151\n",
      "          14       0.95      0.81      0.87       155\n",
      "          15       0.76      0.89      0.82       159\n",
      "          16       0.83      0.84      0.84       140\n",
      "          17       0.85      0.78      0.81       149\n",
      "          18       0.71      0.73      0.72       138\n",
      "          19       0.63      0.55      0.59       101\n",
      "\n",
      "    accuracy                           0.77      2829\n",
      "   macro avg       0.77      0.77      0.77      2829\n",
      "weighted avg       0.78      0.77      0.77      2829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing the classification report for our own naive bayes classifier.\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
